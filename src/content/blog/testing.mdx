---
title: "Testing"
description: "And become a better engineer"
pubDate: "Mar 21 2024"
headerColor: "#7fd8be"
contrastColor: "#f48498"
heroImage: "/blog/test/header-icon.svg"
---

import Cookie from '../../components/Cookie.astro'

Although there are [many forms of testing](https://medium.com/codex/types-of-testing-de4cdd98df77), in this article I will focus on unit-testing. I believe they are the easiest to setup, easy to write/maintain and when done properly [end-to-end testing may become obsolete](https://www.youtube.com/watch?v=QFCHSEHgqFE).

---

> Engineers conduct tests and experiments to evaluate the performance, safety, and reliability of products or systems. This may involve using specialized equipment, conducting simulations, or performing real-world trials.
>
> <cite>[ChatGPT](https://chat.openai.com/share/97ddf631-1171-4c18-9f70-40e2b6f30e8b)</cite>

## The software engineer

In the physical world we rely on engineers to be due diligence in their work. We wouldn't like to have a bridge collapse because it was not tested properly, right?

So how is it that I come across many _software engineers_ that do not test their code?

It is our job that the code we write works as intended, and -preferably- keeps on working as intended after changes have been made.

This is, to me, part of the _engineering_ mindset.

### Some ðŸš©ðŸš©ðŸš©ðŸš©

Every project (-team) is different, but there are some common pitfalls that I have seen over and over again which should raise some red flags to you.

#### ðŸš© No tests runner has been setup

This one is kind of obvious, but when a project has no test runner setup it does not motivate (new) team members to write tests.

Why would you bother writing tests as this project cleary does not find them important?

This is true for a local setup which enables developers to validate their code locally. But it is equaly important to have a test runner in your CI/CD pipeline.

```yml
# Simple github-actions example
name: Node.js CI

on:
  pull_request:
    branches: [main] # Can extend to other branches

jobs:
  is-code-okay:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Use Node.js 18
        uses: actions/setup-node@v2
        with:
          node-version: 18
      - run: npm ci
      - run: npm run test
```

This way you can ensure no old code is broken unintentionally. Even when you forget about running the tests locally.

#### ðŸš© Tests are treated as second class citizens

When tests are stored away in a seperate a folder it makes them harder to find and easier to forget about.

There is no constant nudge to write tests.

```
project-root/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ app.ts
â”‚   â””â”€â”€ utils.ts
â”‚
â””â”€â”€ test/
    â”œâ”€â”€ app.test.ts
    â””â”€â”€ utils.test.ts
```

Try placing the tests as part of the source code, next to the code they are testing.

This way the project itself tells you _"we care about tests"_ just by working on it.

```
project-root/
â”‚
â””â”€â”€ src/
    â”œâ”€â”€ app.ts
    â”œâ”€â”€ app.test.ts
    â”œâ”€â”€ utils.ts
    â””â”€â”€ utils.test.ts
```

#### ðŸš© Test code is not <em class="ml-2">clean</em>

Why do we go to great lengths to write clean, maintainable code, but throw all of this out of the window when writing tests?

Test code should adhere to the same standards of quality as the code it is testing.

> Indeed, the ratio of time spent reading versus writing is well over 10 to 1. We are constantly reading old code as part of the effort to write new code. ...[Therefore,] making it easy to read makes it easier to write.
>
> <cite>Robert C. Martin, Clean Code: A Handbook of Agile Software Craftsmanship</cite>

> Any fool can write code that a computer can understand. Good programmers write code that humans can understand.
>
> <cite>Refactoring: Improving the Design of Existing Code</cite>

---

Some indicators I have found that you throw out clean code principles when writing tests:

---

1. Ignoring test files in your linting

Sure, this makes your life easier in the short term. But over the life-span of a project you will end up with a lot of unmaintainable tests (read technical debt) you have created yourself unnecessarily.

```typescript
{
    "ignorePatterns": ["**/*.test.ts"],
}
```

2. Making the test (file) a puzzle by itself

Tests can be considered documentation itself when written properly.

Why make it harder for the next person, which is most likey going to be you, to understand what is going on?

```typescript
it("should do the thing", () => {
  const a = [1, 10];
  const b = [2, 50];

  // @ts-ignore
  const d = calculateDistance(a, b);
  const c = plotCurve(d, {} as unknown as MyOptions);

  expect(c).toBe([
    [1, 20],
    [7, 18],
    [3, 49],
  ]);
});
```

Act like you are writing _"normal"_ code.

This makes it easier to scim over the tests to see what is being tested and what the expected outcome is.

A good pattern to follow is the [_Arrange, Act, Assert_](https://automationpanda.com/2020/07/07/arrange-act-assert-a-pattern-for-writing-good-tests/) pattern.

```typescript
it("should plot a curce between two data points", () => {
  // Arrange
  const pointA = [1, 10];
  const pointB = [2, 50];
  const expectedCurve = [
    [1, 20],
    [7, 18],
    [3, 49],
  ];

  // Act
  const distance = calculateDistance(pointA, pointB, true);
  const result = plotCurve(distance, { step: 3 });

  // Assert
  expect(result).toEqual(expectedCurve);
});
```

3. The test has too many responsibilities

When a test is _doing_ too many things at once it becomes harder to pinpoint what is going wrong when the tests start failing.

```typescript
it("should test everything", () => {
  // Should set global `someGlobalValue` to `true`
  const result = doSomething("foo", process.env.BAR); // Relies on local DB

  expect(result).toBe(false);
  expect(myMethod).toHaveBeenCalledWith({ foo: "foo", bar: "bar" });
  expect(myOtherMethod).not.toHaveBeenCalled();
});

it("should test some other thing", () => {
  // Only works when the previous test has been run
  const result = doSomeOtherThing(someGlobalValue, 5);

  expect(result).toBe("foo");
});
```

Tests should be

- a) **independant** - `it("should not rely on the outcome of another test")`
- b) **repeatable** - `it("should be runnable in any environment")`
- c) **self-validating** - `it("should return a clear pass or fail result")`

```typescript
jest.mock("myLocaDbAdapter"); // Make sure the tests can run anywhere

describe(doSomething.name, () => {
  it.each`
    input1   | input2   | expected
    ${"foo"} | ${"bar"} | ${false}
    ${"bar"} | ${"foo"} | ${true}
  `(
    "Returns $expected for $input1 and $input2",
    ({ input1, input2, expected }) => {
      const result = doSomething(input1, input2);

      expect(result).toBe(expected);
    }
  );

  it(`does should call ${myMethod.name}`, () => {
    doSomething("foo", "bar");

    expect(myMethod).toHaveBeenCalledWith({ foo: "foo", bar: "bar" });
  });

  it(`does not call ${myOtherMethod.name} when passing "foo" and "bar"`, () => {
    doSomething("foo", "bar");

    expect(myOtherMethod).not.toHaveBeenCalled();
  });

  it(`does call ${myOtherMethod.name} when passing "bar" and "foo"`, () => {
    doSomething("bar", "foo");

    expect(myOtherMethod).toHaveBeenCalledTimes(1);
  });
});

describe(doSomeOtherThing.name, () => {
  it.each`
    someGlobalValue | expected
    ${true}         | ${"foo"}
    ${false}        | ${"bar"}
  `(
    "Returns '$expected' when `someGlobalValue` is $someGlobalValue",
    ({ expected, $someGlobalValue }) => {
      // We don't rely on the outcome of the previous test
      jest.spyOn(global, "someGlobalValue").mockReturnValue(someGlobalValue);

      const result = doSomeOtherThing(someGlobalValue, 5);

      expect(result).toBe(expected);
    }
  );
});
```

Yes, the test file is significantly longer than the original example. But it is easier to follow the intents of each test. And as a bonus, when a test fails you can pinpoint the issue much faster.

#### ðŸš© Coverage !== Confidence

Hitting a certain percentage of coverage does not mean your software is properly tested.

The following code has a `80%` coverage, as this seems to be the general (mandatory) coverage goal.

```typescript
// src/test-me.ts
import { deleteImportantData } from "./utils";

class TestMe {
  private shouldDeleteData: boolean = false;

  constructor(
    public readonly foo: number,
    public readonly bar: string,
    public readonly baz: boolean
  ) {
    if (foo > 10) {
      this.shouldDeleteData = true;
    }
  }

  public importantMethodToTest(): void {
    if (this.shouldDeleteData) {
      deleteImportantData();
    }
  }
}

// src/test-me.test.ts
import { TestMe } from "./test-me";

describe(TestMe.name, () => {
  it("initializes properly", () => {
    const testMe = new TestMe(11, "bar", true);

    expect(testMe.foo).toBe(11);
    expect(testMe.bar).toBe("bar");
    expect(testMe.baz).toBe(true);
  });
});
```

But what are we actually testing here? If the JavaScript constructor still works?

Let's have a look at the next example:

```typescript
// src/add.ts
function add(array: number[]) => {
  return array[0] + array[1];
}

// src/add.test.ts
describe(add.name, () => {
  it("should add two numbers", () => {
    const result = add([1, 2]);

    expect(result).toBe(3);
  });
});
```

And boom, we have `100%` coverage. But we are missing out on testing the edge cases. What if the array is empty? What if the array only has one element?

Setting a (mandatory) coverage percentage is a ~stupid~ bad idea. More often than not this will lead to poorer tests just for the sake of increasing the coverage.

I would not go as far as to [not write any unit tests](https://www.youtube.com/watch?v=ZGKGb109-I4) though, just be [thoughtful on _what_ you are testing](https://www.youtube.com/watch?v=IInciWyU74U).

---

Wow, you made it this far, have a <Cookie name="testing" /> before we continue.

---

## Using specialized equipment

... blurp about tools I use

[zod](https://zod.dev)

[vitest](https://vitest.dev)

[`@total-typescript/shoehorn`](https://www.npmjs.com/package/@total-typescript/shoehorn)

[`@testing-library`](https://testing-library.com)

[codecov](https://about.codecov.io)

## Conducting simulations

... testing can force your system into states

1. throwing errors
2. unexpected inputs
3. ....?

## Performing real-world trials

... unit testing is not all, bla bla bla

Unit testing is great, but you are testing in a vacuum. Software does not exist in a vacuum.

Either do use e2e, smoke, integration tests or have a good QA team.

And if a issue is found, make a test for it so it does not happen again.
